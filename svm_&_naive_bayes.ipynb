{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) What is a Support Vector Machine (SVM)4\n",
        "\n",
        " A supervised ML algorithm that finds the optimal hyperplane that maximizes the margin between different classes in the feature space.\n",
        "\n",
        "2) What is the difference between Hard Margin and Soft Margin SVM4\n",
        "\n",
        " Hard Margin: No misclassifications allowed; works only if data is perfectly linearly separable.\n",
        "\n",
        " Soft Margin: Allows some misclassification by introducing a penalty; better for noisy or overlapping data\n",
        "\n",
        "3) What is the mathematical intuition behind SVM4\n",
        "\n",
        " Find a hyperplane\n",
        "ùë§\n",
        "‚ãÖ\n",
        "ùë•\n",
        "+\n",
        "ùëè\n",
        "=\n",
        "0\n",
        "w‚ãÖx+b=0 that maximizes the margin\n",
        "2\n",
        "‚à•\n",
        "ùë§\n",
        "‚à•\n",
        "‚à•w‚à•\n",
        "2\n",
        "‚Äã\n",
        "  while minimizing classification errors.\n",
        "\n",
        " 4) What is the role of Lagrange Multipliers in SVM4\n",
        "\n",
        "  Used in optimization to transform the constrained problem into an unconstrained one via the dual formulation, enabling use of kernels.\n",
        "\n",
        " 5) What are Support Vectors in SVM4\n",
        "  \n",
        "  The data points that lie closest to the decision boundary; they define the position and orientation of the hyperplane.\n",
        "\n",
        " 6) What is a Support Vector Classifier (SVC)4\n",
        "\n",
        "  An implementation of SVM for classification tasks, possibly using kernels for non-linear decision boundaries.\n",
        "\n",
        " 7)  What is a Support Vector Regressor (SVR)4\n",
        "  \n",
        "  An SVM variant for regression; predicts continuous values while ignoring small errors within an epsilon margin.\n",
        "\n",
        "  8) \u001dWhat is the Kernel Trick in SVM4\n",
        "\n",
        "   Transforms data into a higher-dimensional space without explicitly computing the transformation, enabling non-linear classification.\n",
        "\n",
        " 9) Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "\n",
        "  Linear vs Polynomial vs RBF Kernel\n",
        "Linear:\n",
        "\n",
        "Works well for linearly separable data; fast.\n",
        "\n",
        "Polynomial: Models curved relationships; degree parameter controls flexibility.\n",
        "\n",
        "RBF: Handles complex, non-linear boundaries; uses distance-based similarity.\n",
        "\n",
        "10) What is the effect of the C parameter in SVM4\n",
        "\n",
        " High C: Less tolerance for errors (overfits).\n",
        "\n",
        "Low C: More tolerance for errors (underfits, but better generalization).\n",
        "\n",
        "11) What is the role of the Gamma parameter in RBF Kernel SVM4\n",
        "\n",
        " Controls influence of individual points:\n",
        "\n",
        "High gamma: Each point has narrow influence (overfits).\n",
        "\n",
        "Low gamma: Each point has broad influence (underfits).\n",
        "\n",
        "12) What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"4\n",
        "\n",
        " A probabilistic classifier based on Bayes‚Äô Theorem with the assumption of feature independence (‚Äúna√Øve‚Äù assumption).\n",
        "\n",
        "13) What is Bayes‚Äô Theorem4\n",
        "\n",
        " Bayes‚Äô Theorem\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        "‚à£\n",
        "ùêµ\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        "‚à£\n",
        "ùê¥\n",
        ")\n",
        "‚ãÖ\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        ")\n",
        "P(A‚à£B)=\n",
        "P(B)\n",
        "P(B‚à£A)‚ãÖP(A)\n",
        "‚Äã\n",
        "\n",
        "14) Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes:\n",
        "\n",
        " Gaussian vs Multinomial vs Bernoulli Na√Øve Bayes\n",
        "Gaussian:\n",
        "\n",
        "Assumes continuous features with normal distribution.\n",
        "\n",
        "Multinomial: Works with count-based features (e.g., word counts).\n",
        "\n",
        "Bernoulli: Works with binary features (presence/absence).\n",
        "\n",
        "15) When should you use Gaussian Na√Øve Bayes over other variants4\n",
        "\n",
        " When features are continuous and approximately normally distributed.\n",
        "\n",
        "16) What are the key assumptions made by Na√Øve Bayes4\n",
        "\n",
        " Features are conditionally independent given the class.\n",
        "\n",
        "Features contribute equally to prediction.\n",
        "\n",
        "17) What are the advantages and disadvantages of Na√Øve Bayes4\n",
        "\n",
        " Advantages of Na√Øve Bayes:\n",
        "\n",
        "Simple, fast, works with small datasets.\n",
        "\n",
        "Performs well in high-dimensional spaces.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Strong independence assumption may not hold.\n",
        "\n",
        "Poor at capturing complex relationships.\n",
        "\n",
        "18) Why is Na√Øve Bayes a good choice for text classification4\n",
        "\n",
        " Text data often has many features (words) that are approximately independent; NB handles high-dimensional sparse data well.\n",
        "\n",
        "19) Compare SVM and Na√Øve Bayes for classification tasks:\n",
        "\n",
        " SVM: Works well with complex, high-dimensional boundaries; slower training.\n",
        "\n",
        "NB: Probabilistic, fast, works well with sparse text data.\n",
        "\n",
        "NB often better for text; SVM better for non-linear complex data.\n",
        "\n",
        "20) How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "\n",
        " Adds a small constant to all counts to avoid zero probabilities for unseen feature-class combinations.\n",
        "\n"
      ],
      "metadata": {
        "id": "n6cllP9qhUM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features (important for SVM performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create an SVM classifier with RBF kernel\n",
        "svm_clf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "\n",
        "# Train the model\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Classifier Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "uQVkIRbZlSEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22 Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies:\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Standardize features for better SVM performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# 5. Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# 6. Print results\n",
        "print(f\"Accuracy with Linear Kernel: {acc_linear:.4f}\")\n",
        "print(f\"Accuracy with RBF Kernel:    {acc_rbf:.4f}\")\n",
        "\n",
        "# 7. Compare\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"Linear kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"Both kernels performed equally well.\")"
      ],
      "metadata": {
        "id": "zMI6C_aulmD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23  Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE):\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = housing.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling (important for SVR)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "# y needs to be reshaped before scaling\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Train the SVR model with RBF kernel\n",
        "svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "svr.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_scaled = svr.predict(X_test_scaled)\n",
        "\n",
        "# Inverse transform predictions to original scale\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)"
      ],
      "metadata": {
        "id": "wP14qnApl4r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24  Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary:\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Generate a synthetic dataset\n",
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=42)\n",
        "\n",
        "# 2. Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Train an SVM classifier with Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1, gamma='scale')\n",
        "svm_poly.fit(X_scaled, y)\n",
        "\n",
        "# 4. Create a meshgrid for decision boundary\n",
        "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# 5. Predict over the grid\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# 6. Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, s=30, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "plt.title(\"SVM with Polynomial Kernel (degree=3)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fZyF8uMjmFjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy:\n",
        "\n",
        "# Gaussian Na√Øve Bayes on Breast Cancer dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Create and train the Gaussian Na√Øve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Na√Øve Bayes Accuracy on Breast Cancer dataset: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "id": "j7Ztx_H3mQfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26 Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "\n",
        "# Multinomial Na√Øve Bayes on 20 Newsgroups dataset\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# 1. Load dataset\n",
        "categories = None  # you can also specify a list of categories if needed\n",
        "train_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "test_data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_data.data)}\")\n",
        "print(f\"Number of test samples: {len(test_data.data)}\")\n",
        "print(f\"Target classes: {train_data.target_names}\\n\")\n",
        "\n",
        "# 2. Convert text to numerical features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(train_data.data)\n",
        "X_test = vectorizer.transform(test_data.data)\n",
        "\n",
        "# 3. Train Multinomial Na√Øve Bayes model\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, train_data.target)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Evaluate model\n",
        "accuracy = metrics.accuracy_score(test_data.target, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(metrics.classification_report(test_data.target, y_pred, target_names=test_data.target_names))\n",
        "\n",
        "# 6. Show some predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"Text: {test_data.data[i][:100]}...\")\n",
        "    print(f\"Predicted: {train_data.target_names[y_pred[i]]}\")\n",
        "    print(f\"Actual: {train_data.target_names[test_data.target[i]]}\\n\")"
      ],
      "metadata": {
        "id": "Qw-sdiXimcsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27 Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load dataset (we'll use only 2 features for easy visualization)\n",
        "X, y = datasets.make_classification(\n",
        "    n_samples=100, n_features=2, n_informative=2,\n",
        "    n_redundant=0, n_clusters_per_class=1, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Define different C values to test\n",
        "C_values = [0.1, 1, 10, 100]\n",
        "\n",
        "# 4. Create a mesh grid for plotting decision boundaries\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 300),\n",
        "    np.linspace(y_min, y_max, 300)\n",
        ")\n",
        "\n",
        "# 5. Train and plot for each C\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for i, C in enumerate(C_values, 1):\n",
        "    model = SVC(kernel='linear', C=C)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Predict for mesh grid points\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot\n",
        "    plt.subplot(2, 2, i)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.title(f\"SVM Decision Boundary (C={C})\")\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ZmykGpjmohd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28  Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with binary features=\n",
        "\n",
        "# Bernoulli Na√Øve Bayes for Binary Classification\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Sample binary dataset\n",
        "# Features: binary (0/1)\n",
        "X = np.array([\n",
        "    [1, 0, 1, 0],\n",
        "    [1, 1, 1, 0],\n",
        "    [0, 0, 1, 1],\n",
        "    [0, 1, 0, 0],\n",
        "    [1, 0, 0, 1],\n",
        "    [0, 1, 1, 0],\n",
        "    [1, 1, 0, 1],\n",
        "    [0, 0, 0, 1]\n",
        "])\n",
        "\n",
        "# Labels: binary classification (0 or 1)\n",
        "y = np.array([1, 1, 0, 0, 1, 0, 1, 0])\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Bernoulli Na√Øve Bayes model\n",
        "model = BernoulliNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Yfs9nQu8m_mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29 Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data=\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. SVM without scaling\n",
        "svm_unscaled = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# 4. Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. SVM with scaling\n",
        "svm_scaled = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# 6. Print results\n",
        "print(\"Accuracy without scaling: {:.4f}\".format(accuracy_unscaled))\n",
        "print(\"Accuracy with scaling: {:.4f}\".format(accuracy_scaled))"
      ],
      "metadata": {
        "id": "0VWZS7JVnMhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and after Laplace Smoothing\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model without Laplace-like smoothing (very small var_smoothing)\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=1e-12)\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "acc_no_smoothing = accuracy_score(y_test, pred_no_smoothing)\n",
        "\n",
        "# Model with Laplace-like smoothing (default var_smoothing)\n",
        "gnb_smoothing = GaussianNB()  # default var_smoothing=1e-9\n",
        "gnb_smoothing.fit(X_train, y_train)\n",
        "pred_smoothing = gnb_smoothing.predict(X_test)\n",
        "acc_smoothing = accuracy_score(y_test, pred_smoothing)\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy without smoothing:\", acc_no_smoothing)\n",
        "print(\"Accuracy with smoothing:\", acc_smoothing)\n",
        "print(\"\\nPredictions without smoothing:\", pred_no_smoothing)\n",
        "print(\"Predictions with smoothing:\", pred_smoothing)"
      ],
      "metadata": {
        "id": "bM0Zg_rgnY6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31 Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)=\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset as an example)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Create SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid = GridSearchCV(svm, param_grid, refit=True, verbose=2, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Predictions using the best model\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Accuracy score\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "VjbLK9eXnoWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32 Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy=\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=2000, n_features=20, n_informative=3, n_redundant=1,\n",
        "                            n_clusters_per_class=1, weights=[0.9], flip_y=0, random_state=42)\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM without class weights\n",
        "svm_no_weights = SVC(kernel='rbf', random_state=42)\n",
        "svm_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = svm_no_weights.predict(X_test)\n",
        "\n",
        "# Step 4: Train SVM with class weights\n",
        "svm_with_weights = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
        "svm_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = svm_with_weights.predict(X_test)\n",
        "\n",
        "# Step 5: Results comparison\n",
        "print(\"Without Class Weights:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_no_weights))\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "print(\"\\nWith Class Weights:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_with_weights))\n",
        "print(classification_report(y_test, y_pred_with_weights))"
      ],
      "metadata": {
        "id": "BW6mIPmCn5Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33 Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data=\n",
        "\n",
        "# Na√Øve Bayes Spam Detection\n",
        "# Using the SMS Spam Collection Dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# ---- 1. Load dataset ----\n",
        "# Dataset format:  label,text\n",
        "# Example:\n",
        "# spam,Free entry in 2 a wkly comp to win FA Cup final...\n",
        "# ham,I'm going to be home soon and i don't want to talk about this stuff anymore...\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Encode labels: ham -> 0, spam -> 1\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# ---- 2. Split into train/test ----\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['message'], df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---- 3. Convert text to numerical features ----\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# ---- 4. Train Na√Øve Bayes classifier ----\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# ---- 5. Predictions ----\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# ---- 6. Evaluation ----\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "i6oE6ePuoGFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34 Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and compare their accuracy=\n",
        "\n",
        "# Import libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm_clf = SVC(kernel='linear', random_state=42)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "svm_preds = svm_clf.predict(X_test)\n",
        "\n",
        "# Train Na√Øve Bayes Classifier\n",
        "nb_clf = GaussianNB()\n",
        "nb_clf.fit(X_train, y_train)\n",
        "nb_preds = nb_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
        "nb_accuracy = accuracy_score(y_test, nb_preds)\n",
        "\n",
        "# Print results\n",
        "print(f\"SVM Classifier Accuracy: {svm_accuracy * 100:.2f}%\")\n",
        "print(f\"Na√Øve Bayes Classifier Accuracy: {nb_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Compare performance\n",
        "if svm_accuracy > nb_accuracy:\n",
        "    print(\"SVM performed better on this dataset.\")\n",
        "elif nb_accuracy > svm_accuracy:\n",
        "    print(\"Na√Øve Bayes performed better on this dataset.\")\n",
        "else:\n",
        "    print(\"Both classifiers performed equally well.\")"
      ],
      "metadata": {
        "id": "XEcfgXEtoTgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35 Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare results=\n",
        "\n",
        "# Feature Selection with Na√Øve Bayes and Comparison\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "categories = ['rec.sport.baseball', 'rec.sport.hockey', 'sci.med', 'sci.space']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# 2. Vectorize the text data\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(data.data)\n",
        "y = data.target\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ---------------- Without Feature Selection ----------------\n",
        "nb_all = MultinomialNB()\n",
        "nb_all.fit(X_train, y_train)\n",
        "y_pred_all = nb_all.predict(X_test)\n",
        "acc_all = accuracy_score(y_test, y_pred_all)\n",
        "\n",
        "# ---------------- With Feature Selection ----------------\n",
        "# Keep top 2000 features based on chi-square score\n",
        "selector = SelectKBest(chi2, k=2000)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "nb_selected = MultinomialNB()\n",
        "nb_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = nb_selected.predict(X_test_selected)\n",
        "acc_selected = accuracy_score(y_test, y_pred_selected)\n",
        "\n",
        "# ---------------- Results ----------------\n",
        "print(f\"Accuracy without feature selection: {acc_all:.4f}\")\n",
        "print(f\"Accuracy with feature selection   : {acc_selected:.4f}\")"
      ],
      "metadata": {
        "id": "LhpfpWK7ojlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36 Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy=\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# One-vs-Rest (OvR) strategy\n",
        "ovr_clf = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "ovr_preds = ovr_clf.predict(X_test)\n",
        "ovr_acc = accuracy_score(y_test, ovr_preds)\n",
        "\n",
        "# One-vs-One (OvO) strategy\n",
        "ovo_clf = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
        "ovo_clf.fit(X_train, y_train)\n",
        "ovo_preds = ovo_clf.predict(X_test)\n",
        "ovo_acc = accuracy_score(y_test, ovo_preds)\n",
        "\n",
        "# Display results\n",
        "print(\"SVM with One-vs-Rest (OvR) Accuracy:\", ovr_acc)\n",
        "print(\"SVM with One-vs-One (OvO) Accuracy:\", ovo_acc)\n",
        "\n",
        "# Which is better?\n",
        "if ovr_acc > ovo_acc:\n",
        "    print(\"OvR performed better.\")\n",
        "elif ovo_acc > ovr_acc:\n",
        "    print(\"OvO performed better.\")\n",
        "else:\n",
        "    print(\"Both performed equally.\")"
      ],
      "metadata": {
        "id": "E6-cTjTCovzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37 Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling for better SVM performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define kernels to test\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "accuracies = {}\n",
        "\n",
        "# Train and evaluate SVM for each kernel\n",
        "for kernel in kernels:\n",
        "    svm = SVC(kernel=kernel, random_state=42)\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "    accuracies[kernel] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Display accuracies\n",
        "print(\"SVM Kernel Comparison on Breast Cancer Dataset:\")\n",
        "for kernel, acc in accuracies.items():\n",
        "    print(f\"{kernel.capitalize()} Kernel Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "j39A5fhCo6AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 38  Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy=\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset (Breast Cancer dataset as example)\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Define the SVM Classifier\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Create Stratified K-Fold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(svm_model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Display results\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "print(\"Average Accuracy: {:.4f}\".format(np.mean(scores)))"
      ],
      "metadata": {
        "id": "0yX3asFWpJIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 39 Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare performance\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# List of different prior probability settings\n",
        "priors_list = [\n",
        "    None,               # Let the model learn from data\n",
        "    [0.5, 0.5],         # Equal priors\n",
        "    [0.7, 0.3],         # Class 0 more probable\n",
        "    [0.3, 0.7]          # Class 1 more probable\n",
        "]\n",
        "\n",
        "print(\"Comparing Na√Øve Bayes with Different Priors:\\n\")\n",
        "for priors in priors_list:\n",
        "    model = GaussianNB(priors=priors)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Priors: {priors} -> Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "Jw_QlHaXpSYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40 Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy=\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------\n",
        "# Model with all features\n",
        "# -------------------\n",
        "svm = SVC(kernel=\"linear\", random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_all = svm.predict(X_test)\n",
        "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
        "\n",
        "# -------------------\n",
        "# RFE for feature selection\n",
        "# -------------------\n",
        "n_features_to_select = 10  # You can tune this\n",
        "rfe = RFE(estimator=SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Transform datasets\n",
        "X_train_rfe = rfe.transform(X_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train model on selected features\n",
        "svm_rfe = SVC(kernel=\"linear\", random_state=42)\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "# -------------------\n",
        "# Compare results\n",
        "# -------------------\n",
        "print(f\"Accuracy with all features ({X.shape[1]} features): {accuracy_all:.4f}\")\n",
        "print(f\"Accuracy after RFE ({n_features_to_select} features): {accuracy_rfe:.4f}\")\n",
        "\n",
        "# Show selected features\n",
        "selected_features = [data.feature_names[i] for i, selected in enumerate(rfe.support_) if selected]\n",
        "print(\"\\nSelected Features after RFE:\")\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "bgObDAR9pfs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41  Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy=\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = SVC(kernel='rbf', random_state=42)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Evaluate using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "6KB0oVwtpqOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 42 Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)=\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training & testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gaussian Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probability scores for Log Loss calculation\n",
        "y_proba = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate Log Loss (Cross-Entropy Loss)\n",
        "loss = log_loss(y_test, y_proba)\n",
        "\n",
        "# Display result\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {loss:.4f}\")"
      ],
      "metadata": {
        "id": "y9oWFTwXp3AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 43 Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "\n",
        "# Import libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset as example)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training & testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "model = SVC(kernel='linear', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.2f}\")\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - SVM Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kwVxAYqmqDCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 44 Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE=\n",
        "\n",
        "# SVM Regressor with MAE Evaluation\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load dataset (California Housing)\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Train the SVR model\n",
        "svr = SVR(kernel='rbf', C=100, gamma=0.1)\n",
        "svr.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Predict and inverse transform predictions\n",
        "y_pred_scaled = svr.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
        "\n",
        "# Evaluate model using MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
      ],
      "metadata": {
        "id": "uFaH6dctqLmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 45 Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train & test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Na√Øve Bayes Classifier\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, label=f'Na√Øve Bayes (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Random guess line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Na√Øve Bayes\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FD8OMTwSqdAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 46  Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load dataset (Breast Cancer dataset for binary classification)\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM classifier with probability estimates\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Get prediction probabilities\n",
        "y_scores = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall curve and average precision\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for SVM Classifier')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e4MMTqXuqqtF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}